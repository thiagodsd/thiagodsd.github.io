I"V\
<p>Há algum tempo venho tentando entender de que se trata inteligência artificial. Após os últimos dois anos talvez a única coisa razoavelmente clara é que essa entidade <em>Inteligência Artificial</em> tem um monte de coisa dentro e essas coisas recebem nomes diferentes quando vistas de perspectivas diferentes. E há muitas perspectivas diferentes. Muitas técnicas podem ser obtidas e entendidas a partir de múltiplos frameworks, seja <em>teoria da informação</em>, <em>otimização</em>, <em>estatística</em>, <em>teoria dos jogos</em> ou mesmo <em>física</em>. <a class="citation" href="#davidmackay2003">[1,2]</a></p>

<p>Como bom sujeito treinado em física, pensei em sistematizar algumas ideias seguindo a estrutura de princípios que combinados e usados com esperteza conduzem a resultados úteis. Porém o zoológico de ideias e técnicas é muito vasto e por isso me pareceu melhor maximizar a entropia e estruturar as coisas em pequenos fragmentos desordenados.</p>

<p>Apesar das referências ao final de cada fragmento, a fonte de quase tudo que segue são minhas anotações das disciplinas sobre o assunto que cursei durante a graduação e do período que estudei para uma prova de um processo seletivo para uma vaga de cientista de dados.</p>

<p><br /><br /><br /></p>

<h2 id="o-hello-world-do-raciocínio-probabilístico">o hello world do raciocínio probabilístico</h2>

<p class="topico">estatística</p>

<p>Do ponto de vista estatístico ambas, regressão e classificação, podem ser pensadas como a busca pela aproximação de uma certa $f(y | x, w)$ que relaciona o conjunto de propriedades $x$ de uma observação à chance de certa informação de interesse $y$ dessa observação possuir determinado valor.</p>

<p>Em uma regressão $y$ é uma variável contínua. No problema de classificação $y$ é discreta. Em particular $y \in \{0, 1\}$ dá origem a problemas de classificação binária.<br />
Fundamentalmente nunca se conhece $p = f(y | x, w)$, então há alguma liberdade na escolha da forma funcional de $p$. Por muitos motivos as funções que compõem a chamada <em>família exponencial</em> costumam ser <em>hors concours</em>. Duas escolhas clássicas são</p>

<script type="math/tex; mode=display">\begin{cases}
p \sim \mathcal{N}(y|w^T \phi(x), \sigma^2)         \nonumber \\
p \sim Bern\left( y | \frac{1}{1+e^{-w^T x}} \right)
\end{cases}</script>

<h2 id="a-regressão-logística">a regressão logística</h2>

<p class="topico">classificação</p>

<p>Nos primórdios nada havia. Então surgiu a regressão linear. Muita gente boa debruçou sobre esse problema, dando origem a por exemplo a técnica dos mínimos quadrados.<br />
A princípio os métodos de regressão só funcionam quando a variável de interesse $y$ – ou <em>“target”</em>, ou <em>“variável resposta”</em>, ou <em>“variável dependente”</em> – é contínua.</p>

<p>Fundamentalmente a ideia na regressão logística é usar técnicas/algoritmos de regressão para resolver problemas de classificação. Daí a esquizofrenia da regressão logística – um método de classificação com “regressão” no nome.</p>

<p>O truque é limitar o resultado da regressão no intervalo $[0, 1]$, para ter cara de probabilidade, e eventualmente definir um valor de probabilidade que separa as classes $0$ e $1$.</p>

<p>Há algumas funções que satisfazem essa necessidade, mas no lugar de sacar uma delas é interessante pensar no problema inverso: como levar algo de $[0,1]$ em $\mathbb{R}$?</p>

<p>Algo com a cara $\mathcal{F}(x) = \frac{1}{x}$ serve para levar para o infinito. Nesse caso:</p>

<script type="math/tex; mode=display">\begin{cases}
\mathcal{F}(p=0) = \infty \nonumber \\
\mathcal{F}(p=1) = 1
\end{cases}</script>

<p>Um detalhe da $\mathcal{F}$ é que probabilidades altas levam a valores pequenos e vice-versa. Uma forma de acalmar os corações mais desesperados dos apostadores do mundo é construí-la como $\mathcal{F}(x) = \frac{1}{\frac{1}{x}-1}$, assim:</p>

<script type="math/tex; mode=display">\begin{cases}
\mathcal{F}(p=0) = 0 \nonumber \\
\mathcal{F}(p=1) = +\infty
\end{cases}</script>

<p>Para atingir o outro infinito a $\mathcal{G}(x) = log(x)$ resolve:</p>

<script type="math/tex; mode=display">\begin{cases}
\mathcal{G}(\mathcal{F}(p=0)) = -\infty \nonumber \\
\mathcal{G}(\mathcal{F}(p=1)) = +\infty
\end{cases}</script>

<p>Essas duas funções $\mathcal{F}(p)$ e $\mathcal{G}(\mathcal{F}(p))$ são chamadas por aí respectivamente de <em><strong>odds</strong></em>, ou “chance”, e <em><strong>logit</strong></em>, que já vi chamarem de “logíto” também.<br />
$\mathcal{G}(\mathcal{F}(p))$ leva probabilidades nos reais, então para para resolver um problema de classificação usando os métodos de regressão, basta aplicar $\mathcal{F}^{-1}(\mathcal{G}^{-1}(\cdot)) \equiv \sigma(\cdot)$ no resultado.<br />
Essa inversa também tem nome: <em><strong>função sigmoide</strong></em></p>

<script type="math/tex; mode=display">\sigma(x) = \frac{1}{1+e^{-x}}</script>

<p><br /></p>

<p>Com respeito ao modelo é interessante observar que os parâmetros de ajuste $w$ não são as probabilidades, senão os logitos. Como $p \sim \sigma(w^T x)$, então</p>

<script type="math/tex; mode=display">\boxed{ w^Tx = log \left( \frac{p}{1-p} \right) } \label{reglog}</script>

<p><br /></p>

<p>Uma das hipóteses da regressão logística é que as observações são distribuídas de forma que $p(y=c) \sim Bern$, com $c \in \{0, 1\}$, ou seja</p>

<script type="math/tex; mode=display">\begin{cases}
 Bern(y=1) = p   \nonumber \\
 Bern(y=0) = 1-p 
\end{cases}</script>

<p>À luz disso é interessante olhar pra $\mathcal{F}$ uma última vez:</p>

<script type="math/tex; mode=display">\text{odds} = \frac{p}{1-p} = \frac{Bern(y=1)}{Bern(y=0)} = \frac{\text{evento}}{\neg\text{evento}} = \frac{p_+}{p_-} =  \frac{\sigma(w^T x)}{1-\sigma(w^T x)} = \frac{\sigma(w^T x)}{\sigma(-w^T x)}  \nonumber</script>

<p><br /></p>
<p style="color: red; font-weight: 600;"> a partir daqui os parágrafos são rascunhos e esboços <a class="citation" href="#davidmackay2003">[1,3,4]</a> </p>
<p><br /></p>

<p>Do ponto de vista prático a $\ref{reglog}$ é a equação que caracteriza a regressão logística binária. Para</p>

<h2 id="construção-axiomática-de-kolmogorov">construção axiomática de kolmogorov</h2>

<p class="topico">probabilidade</p>

<p>Dados eventos $A_1, A_2, \dots ,A_n \in \mathcal{A}$, disjuntos, em que $\mathcal{A}$ é uma $\sigma$-álgebra do conjunto $\Omega$, denotando $P(A_i)$ a probabilidade de $A_i$, segue uma função $P$ definida em $\mathcal{A}$ satisfazendo</p>

<p>A1. $P(A) \geq 0$</p>

<p>A2. $P(\Omega) = 1$</p>

<p>A3. $P \left( \bigcup_{n=1}^{\infty} A_n \right) = \sum_{n=1}^{\infty} P(A_n)$</p>

<p>é chamada <strong>medida de probabilidade</strong> em $\mathcal{A}$ e satisfaz as seguintes propriedades</p>

<p>P1. $P(A^c) = 1 - P(A)$</p>

<p>P2. $0 \leq P(a) \leq 1$</p>

<p>P3. $A_1 \subset A_2 \Rightarrow P(A_1) \leq P(A_2)$</p>

<p>P4. $P \left( \bigcup_{i=1}^{\infty} A_i \right) \leq \sum_{i=1}^{\infty} P(A_i)$</p>

<p>P5. $A_n \uparrow A \Rightarrow P(A_n) \uparrow P(A) \qquad e \qquad A_n \downarrow A \Rightarrow P(A_n) \downarrow P(A)$</p>

<p><br /></p>

<h3 id="probabilidade-condicional">probabilidade condicional</h3>

<p>A probabilidade condicional do evento $A$ dada observação de $B$ pode ser definida como</p>

<script type="math/tex; mode=display">P(A  \mid  B) = \frac{P(A \cap B)}{P(B)} \nonumber</script>

<p>expresando um resultado intuitivo: dados $n$ ensaios independentes de $A$ e $B$, no limite de $n \to \infty$ é razoável que</p>

<script type="math/tex; mode=display">P(A \mid B) \sim \frac{1}{n} \left( \frac{\text{# ocorrências de A e B}}{\text{# ocorrências de B}} \right) \nonumber</script>

<p>Disso segue que</p>

<ul>
  <li>$P(A \cap B) = P(B)P(A \mid B)$</li>
  <li>$P(A \cap B \cap C) = P(A)P(B \mid A)P(C \mid A \cap B)$</li>
</ul>

<p>esse segundo resultando podendo ser generalizado para o <strong>teorema da probabilidade composta</strong>, onde vale que</p>

<script type="math/tex; mode=display">P(A_1 \cap \dots \cap A_n) = P(A_1)P(A_2  \mid  A_1)P(A_3 \mid A_1 \cap A_2) \dots P(A_n  \mid  A_1 \cap \dots \cap A_{n-1}) \nonumber</script>

<p>Dada a disjuntividade dos $A_i$, um evento $B \in \mathcal{A}$ pode ser representado como</p>

<script type="math/tex; mode=display">B = \bigcup_i (A_i \cap B) \nonumber</script>

<p>consequentemente</p>

<script type="math/tex; mode=display">P(B) = \sum_i P(A_i \cap B) = \sum_i P(A_i)P(B \mid A_i) \nonumber</script>

<p><br /></p>

<h3 id="fórmula-de-bayes">fórmula de bayes</h3>

<p>Combinando os resultados anteriores segue a fórmula de Bayes</p>

<script type="math/tex; mode=display">\boxed{ P(A_i  \mid  B) = \frac{P(A_i \cap B)}{P(B)} = \frac{P(A_i)P(B \mid A_i)}{\sum_j P(A_j)P(B \mid A_j)} }</script>

<h3 id="exemplo-1">Exemplo 1</h3>

<p>Uma caixa contém duas moedas honestas e uma com duas caras. Lançando uma delas ao acaso e verificando que o resultado foi cara, qual é a probabilidade da moeda lançada ter sido a de duas caras?</p>

<p>Definindo</p>

<ul>
  <li>$A_1$ como moeda honesta</li>
  <li>$A_2$ como moeda de duas caras</li>
  <li>$B$ como resultado igual a cara</li>
</ul>

<p>a probabilidade deseja é definida por $P(A_2  \mid  B)$. Pela fórmula de Bayes</p>

<script type="math/tex; mode=display">P(A_2  \mid  B) = \frac{P(A_2)P(B \mid A_2)}{P(A_1)P(B \mid A_1) + P(A_2)P(B \mid A_2)} = \frac{\frac{1}{3}1}{\frac{2}{3}\frac{1}{2} + \frac{1}{3}1} = \frac{1}{2}</script>

<h3 id="exemplo-2">Exemplo 2</h3>

<p>Definindo o estado de saúde de uma pessoa como</p>

<ul>
  <li>$d=1$ a pessoa tem a doença $D$</li>
  <li>$d=0$ a pessoa não tem a doença $D$</li>
</ul>

<p>sabendo que um teste $T$ possui 95% de acurácia para detectar a doença em pacientes que de fato possuem a doença $D$, e que esse mesmo teste possui 95% de acurácia para detectar que pacientes saudáveis não possuem a doença $D$.</p>

<p>Se apenas 1% da população mundial possui a doença $D$, qual a chance de alguém obter positivo no teste $T$ e realmente ter a doença $D$?</p>

<p>Denotando por</p>

<ul>
  <li>$t=1$ o resultado do teste sendo positivo</li>
  <li>$t=0$ o resultado do teste sendo negativo</li>
</ul>

<p>seguem das informações de acurácia do teste que</p>

<ul>
  <li>$P(t=1  \mid  d=1) = 0.95$</li>
  <li>$P(t=0  \mid  d=1) = 0.05$</li>
  <li>$P(t=1  \mid  d=0) = 0.05$</li>
  <li>$P(t=0  \mid  d=0) = 0.95$</li>
</ul>

<p>e das informações da população mundial segue que</p>

<ul>
  <li>$P(d=1) = 0.01$</li>
  <li>$P(d=0) = 0.99$</li>
</ul>

<p>então a fórmula de Bayes mostra que</p>

<script type="math/tex; mode=display">P(d=1  \mid  t=1) = \frac{P(t=1 \mid d=1)P(d=1)}{P(t=1 \mid d=1)P(d=1)+P(t=1 \mid d=0)P(d=0)} = 0.16</script>

<h3 id="exemplo-3">Exemplo 3</h3>

<p><img src="../assets/img/img_urnas.png" alt="" /></p>

<p>Dadas $11$ urnas $u=0, u=1, \dots, u={10}$, todas com $10$ bolas, sendo $u$ delas bolas pretas. Alguém escolhe uma das urnas ao acaso e realiza $N$ sorteios com reposição. Se após $N$ sorteios, $n_B$ bolas pretas foram obtidas, qual a probabilidade da urna $u$ ter sido a escolhida?</p>

<p>A probabilidade buscada é <script type="math/tex">P(u  \mid  n_B, N)</script> ou seja, a probabilidade da urna $u$ ter sido escolhida, dado que das $N$ bolas, $n_B$ são pretas.</p>

<p>Pela fórmula de Bayes</p>

<script type="math/tex; mode=display">P(u  \mid  n_B, N) = \frac{P(n_B  \mid  u, N)P(u  \mid  N)}{P(n_B  \mid  N)}.</script>

<p>$P(n_B  \mid  u, N)$ é a probabilidade de se obter $n_B$ bolas pretas dado que $N$ sorteios foram realizados na urna $u$. 
A probabilidade de sortear uma bola preta, na urna $u$, em um sorteio único é <script type="math/tex">\frac{u}{10}</script> enquanto a probabilidade da bola sorteada não ser preta é <script type="math/tex">\left(1 - \frac{u}{10} \right).</script></p>

<p>Em $N$ sorteios, a probabilidade de sortear $n_B$ bolas pretas é dada pelo produto</p>

<script type="math/tex; mode=display">\left[ \left( \frac{u}{10} \right)\left( \frac{u}{10} \right) \dots \left(1 - \frac{u}{10} \right)\left(1 - \frac{u}{10} \right) \right]= \left( \frac{u}{10} \right)^{n_B}\left(1 - \frac{u}{10} \right)^{N-n_B}.</script>

<p>Como a ordem não importa, segue que</p>

<script type="math/tex; mode=display">P(n_B  \mid  u, N) = \frac{N!}{n_B!(N-n_B)!}\left( \frac{u}{10} \right)^{n_B}\left(1 - \frac{u}{10} \right)^{N-n_B}.</script>

<p>$P(u  \mid  N)$ é a probabilidade de escolher a urna $u$ - que essencialmente não depende de N, portanto <script type="math/tex">P(u  \mid  N)=P(u)=\frac{1}{11}.</script></p>

<p>$P(n_B  \mid  N)$ é a probabilidade marginal de $n_B$ - eventualmente chamada de <em>evidência</em>, uma vez que depende das informações dadas. Reescrevendo a evidência usando as <em>regra do produto</em> e <em>regra da soma</em>:</p>

<script type="math/tex; mode=display">P(n_B  \mid  N) = \sum_{u} P(n_B, u  \mid  N) = \sum_{u} P(n_B  \mid  u, N)P(u  \mid  N).</script>

<p>Então, finalmente</p>

<script type="math/tex; mode=display">\boxed{ P(u  \mid  n_B, N) = \frac{1}{11}\frac{1}{P(n_B  \mid  N)} \frac{N!}{n_B!(N-n_B)!}\left( \frac{u}{10} \right)^{n_B}\left(1 - \frac{u}{10} \right)^{N-n_B}}</script>

<p>Visualizando a distribuição dados $N$ e $n_B$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">factorial</span> <span class="k">as</span> <span class="n">fac</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>

<span class="k">def</span> <span class="nf">comb</span><span class="p">(</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span>  <span class="n">fac</span><span class="p">(</span><span class="n">a</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">fac</span><span class="p">(</span><span class="n">b</span><span class="p">)</span><span class="o">*</span><span class="n">fac</span><span class="p">(</span><span class="n">a</span><span class="o">-</span><span class="n">b</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">P</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">nb</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="n">marg</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">11</span><span class="p">)</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span> <span class="p">[</span> <span class="n">comb</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nb</span><span class="p">)</span><span class="o">*</span><span class="p">((</span><span class="n">U</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="n">nb</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">U</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">nb</span><span class="p">)</span> <span class="k">for</span> <span class="n">U</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span> <span class="p">]</span> <span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">11</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">marg</span><span class="p">)</span><span class="o">*</span><span class="n">comb</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">nb</span><span class="p">)</span><span class="o">*</span><span class="p">((</span><span class="n">u</span><span class="o">/</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="n">nb</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="p">(</span><span class="n">u</span><span class="o">/</span><span class="mi">10</span><span class="p">))</span><span class="o">**</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="n">nb</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">sorteios</span><span class="p">,</span> <span class="n">bolas_p</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)],</span> <span class="p">[</span><span class="n">P</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">bolas_p</span><span class="p">,</span> <span class="n">sorteios</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)],</span> <span class="n">width</span><span class="o">=</span><span class="mf">0.0005</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">10.5</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Probabilidade de ser a urna u'</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]);</span>
    
<span class="k">def</span> <span class="nf">l</span><span class="p">(</span><span class="n">sorteios</span><span class="p">,</span> <span class="n">bolas_p</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]):</span>
        <span class="n">L</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="s">'{:.7f}'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">P</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">bolas_p</span><span class="p">,</span> <span class="n">sorteios</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">L</span>
</code></pre></div></div>

<p>Por exemplo $N=10, n_B=6$:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">g</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">6</span><span class="p">)</span>
</code></pre></div></div>

<p><img src="../assets/img/out_0000.png" alt="" /></p>

<p>Visualizando a distribuição em toda sua extensão</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">9</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">([</span><span class="n">l</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)],</span> <span class="n">interpolation</span><span class="o">=</span><span class="s">'nearest'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">'Probabilidade'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">'u'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">'nb'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]);</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">11</span><span class="p">)]);</span>
</code></pre></div></div>

<p><img src="../assets/img/out_0001.png" alt="" /></p>

<p><br /><br /></p>

<h1 id="referências">referências</h1>

<div class="tex2jax_ignore">
  <ol class="bibliography"><li><div style="word-break: break-all;">

MacKay, David J. C..


<span id="davidmackay2003"><em>Information Theory, Inference and Learning Algorithms</em></span>.


</div>
</li>
<li><div style="word-break: break-all;">

Russell, Stuart.


<span id="stuartrussell2015"><em>Artificial Intelligence: A Modern Approach</em></span>.


</div>
</li>
<li><div style="word-break: break-all;">

Domingos, Pedro.


<span id="Domingos2012"><em>A few useful things to know about machine learning</em></span>.


</div>
</li>
<li><div style="word-break: break-all;">

Marmerola, Guilherme Duarte.


<span id="marmerola_calibration_2018"><em>Calibration of probabilities for tree-based models</em></span>.


</div>
</li></ol>
</div>
:ET